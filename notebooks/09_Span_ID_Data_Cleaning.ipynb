{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "170451a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json, random, os, hashlib, re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from collections import Counter, defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efb55fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267,\n",
       " [PosixPath('/data/sundeep/Fandom_SI/data/interim/sections_parsed_money-heist_by_page/1026.jsonl'),\n",
       "  PosixPath('/data/sundeep/Fandom_SI/data/interim/sections_parsed_money-heist_by_page/1031.jsonl'),\n",
       "  PosixPath('/data/sundeep/Fandom_SI/data/interim/sections_parsed_money-heist_by_page/1053.jsonl')])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHANGE THIS to your actual per-page section directory\n",
    "SECTIONS_DIR = Path(\"/data/sundeep/Fandom_SI/data/interim/sections_parsed_money-heist_by_page\").expanduser()\n",
    "\n",
    "page_files = sorted(SECTIONS_DIR.glob(\"*.jsonl\"))\n",
    "len(page_files), page_files[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9e271cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_page_sections_jsonl(page_jsonl: Path) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    with open(page_jsonl, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a31d3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_original_like(\n",
    "    section_rows: List[Dict[str, Any]],\n",
    "    link_types_include: List[str] | None,\n",
    "    section_sep: str = \"\\n\\n\",\n",
    "    validate_anchor_text: bool = True,\n",
    ") -> Tuple[str, List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    texts: List[str] = []\n",
    "    spans: List[Dict[str, Any]] = []\n",
    "\n",
    "    total_links = kept_links = bad_offsets = bad_anchor_mismatch = 0\n",
    "    cursor = 0\n",
    "\n",
    "    for rec in section_rows:\n",
    "        sec_text = rec.get(\"text\") or \"\"\n",
    "        sec_links = rec.get(\"links\") or []\n",
    "\n",
    "        texts.append(sec_text)\n",
    "        sec_base = cursor\n",
    "\n",
    "        for link in sec_links:\n",
    "            total_links += 1\n",
    "            lt = link.get(\"link_type\")\n",
    "\n",
    "            if link_types_include is not None and lt not in link_types_include:\n",
    "                continue\n",
    "\n",
    "            s, e = link.get(\"start\"), link.get(\"end\")\n",
    "            if s is None or e is None:\n",
    "                bad_offsets += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                s, e = int(s), int(e)\n",
    "            except Exception:\n",
    "                bad_offsets += 1\n",
    "                continue\n",
    "\n",
    "            if s < 0 or e <= s or e > len(sec_text):\n",
    "                bad_offsets += 1\n",
    "                continue\n",
    "\n",
    "            if validate_anchor_text:\n",
    "                anchor = link.get(\"anchor_text\") or \"\"\n",
    "                if anchor and sec_text[s:e] != anchor:\n",
    "                    bad_anchor_mismatch += 1\n",
    "\n",
    "            kept_links += 1\n",
    "            spans.append({\"start\": sec_base + s, \"end\": sec_base + e, \"type\": lt or \"link\"})\n",
    "\n",
    "        cursor += len(sec_text)\n",
    "        cursor += len(section_sep)  # <-- always adds, including last section\n",
    "\n",
    "    page_text = section_sep.join(texts)\n",
    "\n",
    "    clipped = [sp for sp in spans if 0 <= sp[\"start\"] < sp[\"end\"] <= len(page_text)]\n",
    "    stats = dict(\n",
    "        total_links=total_links,\n",
    "        kept_links=kept_links,\n",
    "        bad_offsets=bad_offsets,\n",
    "        bad_anchor_mismatch=bad_anchor_mismatch,\n",
    "        num_spans=len(clipped),\n",
    "        num_sections=len(section_rows),\n",
    "        text_len=len(page_text),\n",
    "    )\n",
    "    return page_text, clipped, stats\n",
    "\n",
    "\n",
    "def build_fixed_cursor(\n",
    "    section_rows: List[Dict[str, Any]],\n",
    "    link_types_include: List[str] | None,\n",
    "    section_sep: str = \"\\n\\n\",\n",
    "    validate_anchor_text: bool = True,\n",
    ") -> Tuple[str, List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    texts: List[str] = []\n",
    "    spans: List[Dict[str, Any]] = []\n",
    "\n",
    "    total_links = kept_links = bad_offsets = bad_anchor_mismatch = 0\n",
    "    cursor = 0\n",
    "\n",
    "    n = len(section_rows)\n",
    "    for i, rec in enumerate(section_rows):\n",
    "        sec_text = rec.get(\"text\") or \"\"\n",
    "        sec_links = rec.get(\"links\") or []\n",
    "\n",
    "        texts.append(sec_text)\n",
    "        sec_base = cursor\n",
    "\n",
    "        for link in sec_links:\n",
    "            total_links += 1\n",
    "            lt = link.get(\"link_type\")\n",
    "\n",
    "            if link_types_include is not None and lt not in link_types_include:\n",
    "                continue\n",
    "\n",
    "            s, e = link.get(\"start\"), link.get(\"end\")\n",
    "            if s is None or e is None:\n",
    "                bad_offsets += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                s, e = int(s), int(e)\n",
    "            except Exception:\n",
    "                bad_offsets += 1\n",
    "                continue\n",
    "\n",
    "            if s < 0 or e <= s or e > len(sec_text):\n",
    "                bad_offsets += 1\n",
    "                continue\n",
    "\n",
    "            if validate_anchor_text:\n",
    "                anchor = link.get(\"anchor_text\") or \"\"\n",
    "                if anchor and sec_text[s:e] != anchor:\n",
    "                    bad_anchor_mismatch += 1\n",
    "\n",
    "            kept_links += 1\n",
    "            spans.append({\"start\": sec_base + s, \"end\": sec_base + e, \"type\": lt or \"link\"})\n",
    "\n",
    "        cursor += len(sec_text)\n",
    "        if i != n - 1:\n",
    "            cursor += len(section_sep)  # only between sections\n",
    "\n",
    "    page_text = section_sep.join(texts)\n",
    "\n",
    "    clipped = [sp for sp in spans if 0 <= sp[\"start\"] < sp[\"end\"] <= len(page_text)]\n",
    "    stats = dict(\n",
    "        total_links=total_links,\n",
    "        kept_links=kept_links,\n",
    "        bad_offsets=bad_offsets,\n",
    "        bad_anchor_mismatch=bad_anchor_mismatch,\n",
    "        num_spans=len(clipped),\n",
    "        num_sections=len(section_rows),\n",
    "        text_len=len(page_text),\n",
    "    )\n",
    "    return page_text, clipped, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12df0fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'page': '2610.jsonl',\n",
       "   'sections': 3,\n",
       "   'text_len': 547,\n",
       "   'kept_links': 12,\n",
       "   'orig_num_spans': 12,\n",
       "   'fixed_num_spans': 12,\n",
       "   'orig_clipped': 0,\n",
       "   'fixed_clipped': 0,\n",
       "   'anchor_mismatch': 0,\n",
       "   'bad_offsets': 0},\n",
       "  {'page': '1453.jsonl',\n",
       "   'sections': 1,\n",
       "   'text_len': 183,\n",
       "   'kept_links': 2,\n",
       "   'orig_num_spans': 2,\n",
       "   'fixed_num_spans': 2,\n",
       "   'orig_clipped': 0,\n",
       "   'fixed_clipped': 0,\n",
       "   'anchor_mismatch': 0,\n",
       "   'bad_offsets': 0},\n",
       "  {'page': '2045.jsonl',\n",
       "   'sections': 5,\n",
       "   'text_len': 1020,\n",
       "   'kept_links': 25,\n",
       "   'orig_num_spans': 25,\n",
       "   'fixed_num_spans': 25,\n",
       "   'orig_clipped': 0,\n",
       "   'fixed_clipped': 0,\n",
       "   'anchor_mismatch': 0,\n",
       "   'bad_offsets': 0},\n",
       "  {'page': '2599.jsonl',\n",
       "   'sections': 3,\n",
       "   'text_len': 769,\n",
       "   'kept_links': 22,\n",
       "   'orig_num_spans': 22,\n",
       "   'fixed_num_spans': 22,\n",
       "   'orig_clipped': 0,\n",
       "   'fixed_clipped': 0,\n",
       "   'anchor_mismatch': 0,\n",
       "   'bad_offsets': 0},\n",
       "  {'page': '2609.jsonl',\n",
       "   'sections': 3,\n",
       "   'text_len': 530,\n",
       "   'kept_links': 12,\n",
       "   'orig_num_spans': 12,\n",
       "   'fixed_num_spans': 12,\n",
       "   'orig_clipped': 0,\n",
       "   'fixed_clipped': 0,\n",
       "   'anchor_mismatch': 0,\n",
       "   'bad_offsets': 0}],\n",
       " 20)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_original_vs_fixed(page_file: Path, section_sep=\"\\n\\n\", link_types_include=None, validate_anchor_text=True):\n",
    "    rows = load_page_sections_jsonl(page_file)\n",
    "    if not rows:\n",
    "        return None\n",
    "\n",
    "    t1, sp1, st1 = build_original_like(rows, link_types_include, section_sep, validate_anchor_text)\n",
    "    t2, sp2, st2 = build_fixed_cursor(rows, link_types_include, section_sep, validate_anchor_text)\n",
    "\n",
    "    # If cursor drift exists, original tends to clip spans more\n",
    "    return {\n",
    "        \"page\": page_file.name,\n",
    "        \"sections\": st1[\"num_sections\"],\n",
    "        \"text_len\": st1[\"text_len\"],\n",
    "        \"kept_links\": st1[\"kept_links\"],\n",
    "        \"orig_num_spans\": st1[\"num_spans\"],\n",
    "        \"fixed_num_spans\": st2[\"num_spans\"],\n",
    "        \"orig_clipped\": st1[\"kept_links\"] - st1[\"num_spans\"],\n",
    "        \"fixed_clipped\": st2[\"kept_links\"] - st2[\"num_spans\"],\n",
    "        \"anchor_mismatch\": st1[\"bad_anchor_mismatch\"],\n",
    "        \"bad_offsets\": st1[\"bad_offsets\"],\n",
    "    }\n",
    "\n",
    "sample = random.sample(page_files, min(20, len(page_files)))\n",
    "rows = [compare_original_vs_fixed(p) for p in sample]\n",
    "rows = [r for r in rows if r is not None]\n",
    "rows[:5], len(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47d1399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages where original < fixed (likely cursor/separator drift): 0\n",
      "Pages where equal: 20\n",
      "Max orig_clipped: 0\n",
      "Max fixed_clipped: 0\n",
      "\n",
      "Top suspicious pages:\n",
      "2610.jsonl orig_clipped= 0 fixed_clipped= 0 kept_links= 12\n",
      "1453.jsonl orig_clipped= 0 fixed_clipped= 0 kept_links= 2\n",
      "2045.jsonl orig_clipped= 0 fixed_clipped= 0 kept_links= 25\n",
      "2599.jsonl orig_clipped= 0 fixed_clipped= 0 kept_links= 22\n",
      "2609.jsonl orig_clipped= 0 fixed_clipped= 0 kept_links= 12\n",
      "2448.jsonl orig_clipped= 0 fixed_clipped= 0 kept_links= 14\n",
      "1725.jsonl orig_clipped= 0 fixed_clipped= 0 kept_links= 14\n",
      "1914.jsonl orig_clipped= 0 fixed_clipped= 0 kept_links= 12\n",
      "384.jsonl orig_clipped= 0 fixed_clipped= 0 kept_links= 52\n",
      "1736.jsonl orig_clipped= 0 fixed_clipped= 0 kept_links= 14\n"
     ]
    }
   ],
   "source": [
    "if not rows:\n",
    "    print(\"No pages loaded.\")\n",
    "else:\n",
    "    worse = sum(1 for r in rows if r[\"orig_num_spans\"] < r[\"fixed_num_spans\"])\n",
    "    equal = sum(1 for r in rows if r[\"orig_num_spans\"] == r[\"fixed_num_spans\"])\n",
    "    print(\"Pages where original < fixed (likely cursor/separator drift):\", worse)\n",
    "    print(\"Pages where equal:\", equal)\n",
    "    print(\"Max orig_clipped:\", max(r[\"orig_clipped\"] for r in rows))\n",
    "    print(\"Max fixed_clipped:\", max(r[\"fixed_clipped\"] for r in rows))\n",
    "    sorted_rows = sorted(rows, key=lambda r: (r[\"orig_clipped\"] - r[\"fixed_clipped\"]), reverse=True)\n",
    "    print(\"\\nTop suspicious pages:\")\n",
    "    for r in sorted_rows[:10]:\n",
    "        print(r[\"page\"], \"orig_clipped=\", r[\"orig_clipped\"], \"fixed_clipped=\", r[\"fixed_clipped\"], \"kept_links=\", r[\"kept_links\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ff436c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1737.jsonl spans= 25 issues= {}\n",
      "2566.jsonl spans= 19 issues= {}\n",
      "348.jsonl spans= 70 issues= {}\n",
      "349.jsonl spans= 66 issues= {}\n",
      "1699.jsonl spans= 12 issues= {}\n",
      "840.jsonl spans= 31 issues= {}\n",
      "473.jsonl spans= 14 issues= {}\n",
      "493.jsonl spans= 30 issues= {}\n",
      "2451.jsonl spans= 17 issues= {}\n",
      "417.jsonl spans= 102 issues= {}\n"
     ]
    }
   ],
   "source": [
    "def integrity_checks(page_text: str, spans: List[Dict[str, Any]]):\n",
    "    issues = Counter()\n",
    "\n",
    "    for sp in spans:\n",
    "        s, e = sp[\"start\"], sp[\"end\"]\n",
    "        if not isinstance(s, int) or not isinstance(e, int):\n",
    "            issues[\"non_int_offsets\"] += 1\n",
    "            continue\n",
    "        if s < 0 or e < 0:\n",
    "            issues[\"negative_offsets\"] += 1\n",
    "        if e <= s:\n",
    "            issues[\"non_positive_length\"] += 1\n",
    "        if e > len(page_text):\n",
    "            issues[\"end_out_of_bounds\"] += 1\n",
    "        if s >= len(page_text):\n",
    "            issues[\"start_out_of_bounds\"] += 1\n",
    "        if 0 <= s < e <= len(page_text):\n",
    "            if page_text[s:e] == \"\":\n",
    "                issues[\"empty_substring\"] += 1\n",
    "            if page_text[s:e].isspace():\n",
    "                issues[\"all_whitespace_span\"] += 1\n",
    "\n",
    "    return issues\n",
    "\n",
    "# Run on a few pages using FIXED cursor (recommended ground truth)\n",
    "sample2 = random.sample(page_files, min(10, len(page_files)))\n",
    "for p in sample2:\n",
    "    rows_ = load_page_sections_jsonl(p)\n",
    "    t, sp, st = build_fixed_cursor(rows_, link_types_include=None, section_sep=\"\\n\\n\", validate_anchor_text=True)\n",
    "    issues = integrity_checks(t, sp)\n",
    "    print(p.name, \"spans=\", len(sp), \"issues=\", dict(issues))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e28c6c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 200\n",
      "Pages where original < fixed: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "sample = random.sample(page_files, min(200, len(page_files)))\n",
    "rows = [compare_original_vs_fixed(p) for p in sample]\n",
    "rows = [r for r in rows if r is not None]\n",
    "\n",
    "worse = [r for r in rows if r[\"orig_num_spans\"] < r[\"fixed_num_spans\"]]\n",
    "print(\"Sample size:\", len(rows))\n",
    "print(\"Pages where original < fixed:\", len(worse))\n",
    "if worse:\n",
    "    print(\"Example suspicious page:\", worse[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff86d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 2313.jsonl | spans: 17 | text_len: 524\n",
      "internal 368 375 => 'Tatiana'\n",
      "internal 95 115 => 'Your Place in Heaven'\n",
      "internal 327 333 => 'Berlin'\n",
      "internal 471 480 => 'Marseille'\n",
      "internal 504 524 => 'Your Place in Heaven'\n",
      "internal 495 502 => 'Tatiana'\n",
      "internal 488 494 => 'Rafael'\n",
      "internal 304 324 => 'Your Place in Heaven'\n",
      "internal 254 260 => 'Rafael'\n",
      "internal 229 235 => 'Berlin'\n",
      "internal 240 247 => 'Tatiana'\n",
      "internal 481 487 => 'Bogotá'\n",
      "internal 336 345 => 'Marseille'\n",
      "internal 116 136 => 'Your Place in Heaven'\n",
      "internal 348 354 => 'Bogotá'\n",
      "internal 464 470 => 'Berlin'\n",
      "internal 357 363 => 'Rafael'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def spotcheck_span_substrings(page_file, k=20, section_sep=\"\\n\\n\"):\n",
    "    rows = load_page_sections_jsonl(page_file)\n",
    "    text, spans, st = build_fixed_cursor(rows, None, section_sep, validate_anchor_text=True)\n",
    "\n",
    "    if not spans:\n",
    "        print(page_file.name, \"NO SPANS\")\n",
    "        return\n",
    "\n",
    "    picks = random.sample(spans, min(k, len(spans)))\n",
    "    print(\"Page:\", page_file.name, \"| spans:\", len(spans), \"| text_len:\", len(text))\n",
    "    for sp in picks:\n",
    "        s, e = sp[\"start\"], sp[\"end\"]\n",
    "        frag = text[s:e]\n",
    "        print(sp[\"type\"], s, e, \"=>\", repr(frag))\n",
    "\n",
    "spotcheck_span_substrings(random.choice(page_files), k=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a089ca97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 2608.jsonl mismatches_shown: 0\n"
     ]
    }
   ],
   "source": [
    "def find_anchor_mismatches(page_file: Path, limit=10, section_sep=\"\\n\\n\"):\n",
    "    rows = load_page_sections_jsonl(page_file)\n",
    "    if not rows:\n",
    "        return\n",
    "\n",
    "    mismatches = []\n",
    "    for rec in rows:\n",
    "        sec_text = rec.get(\"text\") or \"\"\n",
    "        for link in (rec.get(\"links\") or []):\n",
    "            s, e = link.get(\"start\"), link.get(\"end\")\n",
    "            anchor = link.get(\"anchor_text\") or \"\"\n",
    "            if not anchor or s is None or e is None:\n",
    "                continue\n",
    "            try:\n",
    "                s, e = int(s), int(e)\n",
    "            except:\n",
    "                continue\n",
    "            if 0 <= s < e <= len(sec_text):\n",
    "                if sec_text[s:e] != anchor:\n",
    "                    mismatches.append((anchor, sec_text[s:e], link.get(\"link_type\")))\n",
    "                    if len(mismatches) >= limit:\n",
    "                        break\n",
    "        if len(mismatches) >= limit:\n",
    "            break\n",
    "\n",
    "    print(\"Page:\", page_file.name, \"mismatches_shown:\", len(mismatches))\n",
    "    for a, sub, lt in mismatches:\n",
    "        print(\"type=\", lt, \"\\n  anchor_text:\", repr(a), \"\\n  substring  :\", repr(sub), \"\\n\")\n",
    "\n",
    "# Try a random page; or replace with a known suspicious one from Cell 6\n",
    "p = random.choice(page_files)\n",
    "find_anchor_mismatches(p, limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f40919c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Builtin hash split distribution: Counter({'train': 162, 'test': 20, 'dev': 18})\n",
      "Stable md5 split distribution  : Counter({'train': 147, 'test': 30, 'dev': 23})\n",
      "\n",
      "NOTE: builtin hash can change between processes unless PYTHONHASHSEED is fixed.\n"
     ]
    }
   ],
   "source": [
    "def split_bucket_builtin_hash(name: str) -> str:\n",
    "    h = abs(hash(name)) % 100\n",
    "    if h < 80: return \"train\"\n",
    "    if h < 90: return \"dev\"\n",
    "    return \"test\"\n",
    "\n",
    "def split_bucket_stable_md5(name: str) -> str:\n",
    "    h = int(hashlib.md5(name.encode(\"utf-8\")).hexdigest(), 16) % 100\n",
    "    if h < 80: return \"train\"\n",
    "    if h < 90: return \"dev\"\n",
    "    return \"test\"\n",
    "\n",
    "keys = [p.stem for p in random.sample(page_files, min(200, len(page_files)))]\n",
    "builtin = Counter(split_bucket_builtin_hash(k) for k in keys)\n",
    "stable = Counter(split_bucket_stable_md5(k) for k in keys)\n",
    "\n",
    "print(\"Builtin hash split distribution:\", builtin)\n",
    "print(\"Stable md5 split distribution  :\", stable)\n",
    "print(\"\\nNOTE: builtin hash can change between processes unless PYTHONHASHSEED is fixed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2a9cead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats: {'suspicious_substring': 624}\n",
      "\n",
      "Suspicious examples (up to 20):\n",
      "321.jsonl {'start': 2441, 'end': 2446, 'type': 'other'} '[ 1 ]'\n",
      "321.jsonl {'start': 2751, 'end': 2756, 'type': 'other'} '[ 1 ]'\n",
      "321.jsonl {'start': 2802, 'end': 2807, 'type': 'other'} '[ 1 ]'\n",
      "2449.jsonl {'start': 1578, 'end': 1583, 'type': 'other'} '[ 1 ]'\n",
      "2449.jsonl {'start': 1968, 'end': 1969, 'type': 'other'} '↑'\n",
      "86.jsonl {'start': 220, 'end': 225, 'type': 'other'} '[ 1 ]'\n",
      "86.jsonl {'start': 1023, 'end': 1028, 'type': 'other'} '[ 2 ]'\n",
      "86.jsonl {'start': 1055, 'end': 1060, 'type': 'other'} '[ 3 ]'\n",
      "86.jsonl {'start': 2307, 'end': 2312, 'type': 'other'} '[ 4 ]'\n",
      "86.jsonl {'start': 8979, 'end': 8980, 'type': 'other'} '↑'\n",
      "86.jsonl {'start': 9014, 'end': 9015, 'type': 'other'} '↑'\n",
      "86.jsonl {'start': 9031, 'end': 9032, 'type': 'other'} '↑'\n",
      "86.jsonl {'start': 9048, 'end': 9049, 'type': 'other'} '↑'\n",
      "707.jsonl {'start': 368, 'end': 373, 'type': 'other'} '[ 1 ]'\n",
      "707.jsonl {'start': 405, 'end': 410, 'type': 'other'} '[ 2 ]'\n",
      "707.jsonl {'start': 1897, 'end': 1898, 'type': 'other'} '↑'\n",
      "707.jsonl {'start': 1914, 'end': 1915, 'type': 'other'} '↑'\n",
      "165.jsonl {'start': 111, 'end': 116, 'type': 'other'} '[ 1 ]'\n",
      "165.jsonl {'start': 188, 'end': 193, 'type': 'other'} '[ 2 ]'\n",
      "165.jsonl {'start': 867, 'end': 872, 'type': 'other'} '[ 3 ]'\n"
     ]
    }
   ],
   "source": [
    "import random, re\n",
    "from collections import Counter\n",
    "\n",
    "def is_suspicious_substring(s: str) -> bool:\n",
    "    # Heuristics for “misaligned but in-bounds”\n",
    "    if not s or s.isspace():\n",
    "        return True\n",
    "    if \"\\n\\n\" in s:  # section separators leaking into span\n",
    "        return True\n",
    "    if len(s) > 200:  # extremely long anchor is suspicious\n",
    "        return True\n",
    "    # weird: begins/ends with whitespace or punctuation-heavy\n",
    "    if s[0].isspace() or s[-1].isspace():\n",
    "        return True\n",
    "    if sum(ch.isalnum() for ch in s) / max(1, len(s)) < 0.4:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_pages(page_files, n_pages=300, section_sep=\"\\n\\n\"):\n",
    "    stats = Counter()\n",
    "    suspicious_examples = []\n",
    "\n",
    "    sample = random.sample(page_files, min(n_pages, len(page_files)))\n",
    "    for p in sample:\n",
    "        rows = load_page_sections_jsonl(p)\n",
    "        text, spans, st = build_fixed_cursor(rows, None, section_sep, validate_anchor_text=True)\n",
    "\n",
    "        # core invariants\n",
    "        for sp in spans:\n",
    "            s, e = sp[\"start\"], sp[\"end\"]\n",
    "            if not (isinstance(s, int) and isinstance(e, int)):\n",
    "                stats[\"non_int\"] += 1\n",
    "                continue\n",
    "            if not (0 <= s < e <= len(text)):\n",
    "                stats[\"out_of_bounds\"] += 1\n",
    "                continue\n",
    "\n",
    "            frag = text[s:e]\n",
    "            if is_suspicious_substring(frag):\n",
    "                stats[\"suspicious_substring\"] += 1\n",
    "                if len(suspicious_examples) < 20:\n",
    "                    suspicious_examples.append((p.name, sp, frag))\n",
    "\n",
    "        # page-level checks\n",
    "        if st[\"kept_links\"] and st[\"num_spans\"] < st[\"kept_links\"]:\n",
    "            stats[\"pages_with_clipping\"] += 1\n",
    "\n",
    "    return stats, suspicious_examples\n",
    "\n",
    "stats, examples = check_pages(page_files, n_pages=300)\n",
    "print(\"Stats:\", dict(stats))\n",
    "print(\"\\nSuspicious examples (up to 20):\")\n",
    "for ex in examples:\n",
    "    print(ex[0], ex[1], repr(ex[2]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fandom-span",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
