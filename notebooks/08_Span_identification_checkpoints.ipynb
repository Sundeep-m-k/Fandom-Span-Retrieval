{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29f7317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fandom Span Identification — Checkpoints & Sanity Checks\n",
    "\n",
    "# This notebook validates the **span identification (hyperlink detection)** model trained\n",
    "# on Fandom pages.\n",
    "\n",
    "# Pipeline validated:\n",
    "# HTML → char spans → BILOU → token classification → predicted spans\n",
    "\n",
    "# Repo root:\n",
    "# `/data/sundeep/Fandom_SI`\n",
    "\n",
    "# Expected:\n",
    "# - Model + tokenizer load correctly\n",
    "# - Predicted spans are readable anchor text\n",
    "# - Span-level metrics are non-zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ea6be67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOMAIN: money-heist\n",
      "MODEL_DIR: /data/sundeep/Fandom_SI/experiments/span_id/money-heist/bert-base-uncased/checkpoint-81\n",
      "DATA_DIR: /data/sundeep/Fandom_SI/data/processed_span_identi/money-heist\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG ===\n",
    "DOMAIN = \"money-heist\"\n",
    "MODEL_DIR = \"/data/sundeep/Fandom_SI/experiments/span_id/money-heist/bert-base-uncased/checkpoint-81\"\n",
    "DATA_DIR = \"/data/sundeep/Fandom_SI/data/processed_span_identi/money-heist\"\n",
    "MAX_LEN = 256\n",
    "\n",
    "print(\"DOMAIN:\", DOMAIN)\n",
    "print(\"MODEL_DIR:\", MODEL_DIR)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "\n",
    "# EXPECTED OUTPUT:\n",
    "# DOMAIN: money-heist\n",
    "# MODEL_DIR: experiments/span_id/money-heist/bert-base-uncased\n",
    "# DATA_DIR: data/processed_span_identi/money-heist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30d87f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found: 11\n",
      " - config.json\n",
      " - model.safetensors\n",
      " - optimizer.pt\n",
      " - rng_state.pth\n",
      " - scheduler.pt\n",
      " - special_tokens_map.json\n",
      " - tokenizer.json\n",
      " - tokenizer_config.json\n",
      " - trainer_state.json\n",
      " - training_args.bin\n",
      " - vocab.txt\n",
      "\n",
      "Checks:\n",
      "config.json: True\n",
      "model weights: True\n",
      "tokenizer_config.json: True\n",
      "special_tokens_map.json: True\n",
      "vocab.txt: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(MODEL_DIR)\n",
    "assert p.exists(), f\"Missing model dir: {p}\"\n",
    "\n",
    "files = {x.name for x in p.glob(\"*\")}\n",
    "print(\"Files found:\", len(files))\n",
    "for f in sorted(files):\n",
    "    print(\" -\", f)\n",
    "\n",
    "print(\"\\nChecks:\")\n",
    "print(\"config.json:\", \"config.json\" in files)\n",
    "print(\"model weights:\", any(x in files for x in [\"pytorch_model.bin\", \"model.safetensors\"]))\n",
    "print(\"tokenizer_config.json:\", \"tokenizer_config.json\" in files)\n",
    "print(\"special_tokens_map.json:\", \"special_tokens_map.json\" in files)\n",
    "print(\"vocab.txt:\", \"vocab.txt\" in files)\n",
    "\n",
    "# EXPECTED:\n",
    "# All checks should print True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b525b262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: BertTokenizerFast\n",
      "Num labels: 5\n",
      "id2label: {0: 'O', 1: 'B', 2: 'I', 3: 'L', 4: 'U'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "\n",
    "print(\"Tokenizer:\", tok.__class__.__name__)\n",
    "print(\"Num labels:\", model.config.num_labels)\n",
    "print(\"id2label:\", model.config.id2label)\n",
    "\n",
    "# EXPECTED:\n",
    "# Tokenizer: BertTokenizerFast\n",
    "# Num labels: 5\n",
    "# id2label: {0:'O',1:'B-LINK',2:'I-LINK',3:'L-LINK',4:'U-LINK'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d173351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: docs=212, spans=5419\n",
      "dev: docs=27, spans=757\n",
      "test: docs=28, spans=541\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_jsonl(p):\n",
    "    rows=[]\n",
    "    with open(p,\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "train = read_jsonl(Path(DATA_DIR)/\"train.jsonl\")\n",
    "dev   = read_jsonl(Path(DATA_DIR)/\"dev.jsonl\")\n",
    "test  = read_jsonl(Path(DATA_DIR)/\"test.jsonl\")\n",
    "\n",
    "def stats(name, rows):\n",
    "    spans=sum(len(r[\"spans\"]) for r in rows)\n",
    "    print(f\"{name}: docs={len(rows)}, spans={spans}\")\n",
    "\n",
    "stats(\"train\", train)\n",
    "stats(\"dev\", dev)\n",
    "stats(\"test\", test)\n",
    "\n",
    "# EXPECTED (approx):\n",
    "# train: docs=212, spans~5400\n",
    "# dev: docs=27, spans~700\n",
    "# test: docs=28, spans~500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41380064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id: money-heist||159\n",
      "Gold spans: 72\n",
      "\n",
      "Predicted non-O tokens:\n",
      "U       -> 'Denver'\n",
      "B       -> 'Daniel'\n",
      "U       -> 'Tokyo'\n",
      "U       -> 'Moscow'\n",
      "U       -> 'Benjamín'\n",
      "U       -> 'Stockholm'\n",
      "U       -> 'Manila'\n",
      "U       -> 'Denver'\n",
      "B       -> 'Miguel'\n",
      "I       -> 'Fernández'\n",
      "I       -> 'ani'\n",
      "L       -> 'lla'\n",
      "U       -> 'Moscow'\n",
      "U       -> 'Cincinnati'\n",
      "U       -> 'Manila'\n",
      "U       -> 'Vane'\n",
      "U       -> 'Tokyo'\n",
      "U       -> 'Stockholm'\n",
      "U       -> 'Manila'\n",
      "B       -> 'The'\n",
      "L       -> 'Professor'\n",
      "U       -> 'Helsinki'\n",
      "U       -> 'Tokyo'\n",
      "U       -> 'Nairobi'\n",
      "U       -> 'Stockholm'\n",
      "U       -> 'Rio'\n",
      "U       -> 'Bogotá'\n",
      "U       -> 'Palermo'\n",
      "U       -> 'Berlin'\n",
      "U       -> 'Oslo'\n",
      "U       -> 'Lisbon'\n",
      "B       -> 'Mat'\n",
      "I       -> 'ías'\n",
      "I       -> 'Cañ'\n",
      "L       -> 'o'\n",
      "U       -> 'Manila'\n",
      "B       -> 'Miguel'\n",
      "I       -> 'Fernández'\n",
      "I       -> 'Tal'\n",
      "I       -> 'ani'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "ex = random.choice(dev)\n",
    "text = ex[\"text\"]\n",
    "\n",
    "enc = tok(\n",
    "    text,\n",
    "    return_offsets_mapping=True,\n",
    "    truncation=True,\n",
    "    max_length=MAX_LEN,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(\n",
    "        input_ids=enc[\"input_ids\"],\n",
    "        attention_mask=enc[\"attention_mask\"]\n",
    "    ).logits\n",
    "\n",
    "pred_ids = logits.argmax(-1)[0].tolist()\n",
    "offsets = enc[\"offset_mapping\"][0].tolist()\n",
    "\n",
    "id2label = model.config.id2label\n",
    "def lab(i):\n",
    "    return id2label[i] if i in id2label else id2label[str(i)]\n",
    "\n",
    "print(\"doc_id:\", ex[\"doc_id\"])\n",
    "print(\"Gold spans:\", len(ex[\"spans\"]))\n",
    "print(\"\\nPredicted non-O tokens:\")\n",
    "\n",
    "shown=0\n",
    "for (s,e), pid in zip(offsets, pred_ids):\n",
    "    if s==0 and e==0: continue\n",
    "    if lab(pid)!=\"O\":\n",
    "        print(f\"{lab(pid):7} -> {text[s:e]!r}\")\n",
    "        shown+=1\n",
    "    if shown>=40: break\n",
    "\n",
    "# EXPECTED:\n",
    "# B-LINK / I-LINK tokens with readable text like:\n",
    "# 'Berlin', 'Professor', 'Tokyo'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b451ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilou_to_spans(tags, offsets):\n",
    "    \"\"\"\n",
    "    Decode B/I/L/U/O OR B-LINK/I-LINK/... into character spans\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    cur_start = None\n",
    "    cur_end = None\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur_start, cur_end\n",
    "        if cur_start is not None and cur_end is not None and cur_end > cur_start:\n",
    "            spans.append((cur_start, cur_end))\n",
    "        cur_start = None\n",
    "        cur_end = None\n",
    "\n",
    "    for (s, e), t in zip(offsets, tags):\n",
    "        if s == 0 and e == 0:\n",
    "            continue\n",
    "\n",
    "        t0 = t.split(\"-\", 1)[0]  # <-- IMPORTANT FIX\n",
    "\n",
    "        if t0 == \"U\":\n",
    "            spans.append((s, e))\n",
    "        elif t0 == \"B\":\n",
    "            flush()\n",
    "            cur_start, cur_end = s, e\n",
    "        elif t0 == \"I\":\n",
    "            if cur_start is None:\n",
    "                cur_start, cur_end = s, e\n",
    "            else:\n",
    "                cur_end = e\n",
    "        elif t0 == \"L\":\n",
    "            if cur_start is None:\n",
    "                spans.append((s, e))\n",
    "            else:\n",
    "                cur_end = e\n",
    "                flush()\n",
    "        else:  # O\n",
    "            flush()\n",
    "\n",
    "    flush()\n",
    "    return spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9344ec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred spans: 0\n",
      "First 10:\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred spans:\", len(pred_spans))\n",
    "print(\"First 10:\")\n",
    "for s,e in pred_spans[:10]:\n",
    "    print(text[s:e])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c21a1f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 0 FP: 0 FN: 72\n",
      "Precision: 0 Recall: 0.0 F1: 0\n"
     ]
    }
   ],
   "source": [
    "gold={(s[\"start\"],s[\"end\"]) for s in ex[\"spans\"]}\n",
    "pred=set(pred_spans)\n",
    "\n",
    "tp=len(gold & pred)\n",
    "fp=len(pred - gold)\n",
    "fn=len(gold - pred)\n",
    "\n",
    "prec=tp/(tp+fp) if tp+fp else 0\n",
    "rec=tp/(tp+fn) if tp+fn else 0\n",
    "f1=2*prec*rec/(prec+rec) if prec+rec else 0\n",
    "\n",
    "print(\"TP:\",tp,\"FP:\",fp,\"FN:\",fn)\n",
    "print(\"Precision:\",round(prec,3),\"Recall:\",round(rec,3),\"F1:\",round(f1,3))\n",
    "\n",
    "# EXPECTED:\n",
    "# F1 > 0 for many docs (not necessarily high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce32b96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer class: BertTokenizerFast\n",
      "tok.is_fast: True\n",
      "Total tokens: 239\n",
      "Real-offset tokens: 237\n",
      "First 20 offsets: [[0, 0], [0, 4], [4, 6], [7, 9], [9, 14], [15, 27], [28, 39], [40, 46], [47, 53], [54, 60], [61, 66], [67, 80], [81, 87], [88, 90], [90, 94], [95, 97], [97, 101], [101, 102], [103, 104], [104, 112]]\n",
      "First 30 tags: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'L', 'O', 'O', 'O', 'B', 'I', 'I', 'L', 'O', 'O', 'O', 'B', 'I']\n",
      "Tag counts: {'B': 11, 'I': 24, 'L': 13, 'O': 188, 'U': 3}\n"
     ]
    }
   ],
   "source": [
    "import json, torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "MODEL_DIR = \"/data/sundeep/Fandom_SI/experiments/span_id/money-heist/bert-base-uncased/checkpoint-81\"  # <-- set your real checkpoint\n",
    "DATA_DIR  = \"/data/sundeep/Fandom_SI/data/processed_span_identi/money-heist\"\n",
    "MAX_LEN   = 256\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "\n",
    "print(\"Tokenizer class:\", tok.__class__.__name__)\n",
    "print(\"tok.is_fast:\", getattr(tok, \"is_fast\", False))\n",
    "\n",
    "ex = json.loads(open(f\"{DATA_DIR}/dev.jsonl\",\"r\",encoding=\"utf-8\").readline())\n",
    "text = ex[\"text\"]\n",
    "\n",
    "enc = tok(text, return_offsets_mapping=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "offsets = enc[\"offset_mapping\"][0].tolist()\n",
    "\n",
    "# count how many tokens have real offsets\n",
    "real = [(s,e) for (s,e) in offsets if not (s==0 and e==0)]\n",
    "print(\"Total tokens:\", len(offsets))\n",
    "print(\"Real-offset tokens:\", len(real))\n",
    "print(\"First 20 offsets:\", offsets[:20])\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids=enc[\"input_ids\"], attention_mask=enc[\"attention_mask\"]).logits\n",
    "pred_ids = logits.argmax(-1)[0].tolist()\n",
    "\n",
    "id2label = model.config.id2label\n",
    "def lab(i):\n",
    "    if i in id2label: return id2label[i]\n",
    "    if str(i) in id2label: return id2label[str(i)]\n",
    "    return str(i)\n",
    "\n",
    "tags = [lab(i) for i in pred_ids]\n",
    "print(\"First 30 tags:\", tags[:30])\n",
    "print(\"Tag counts:\", {t: tags.count(t) for t in sorted(set(tags))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c518a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred spans: 18\n",
      "First 10:\n",
      "[88:102] -> 'Raquel Murillo'\n",
      "[114:127] -> 'Laura Murillo'\n",
      "[139:151] -> 'Paula Vicuña'\n",
      "[152:159] -> 'Murillo'\n",
      "[176:190] -> 'Alberto Vicuña'\n",
      "[234:247] -> 'The Professor'\n",
      "[279:280] -> '2'\n",
      "[283:284] -> '3'\n",
      "[287:288] -> '4'\n",
      "[302:305] -> 'Kit'\n"
     ]
    }
   ],
   "source": [
    "# --- guaranteed BILOU decoder for tags: B/I/L/U/O (or B-LINK etc.) ---\n",
    "def bilou_to_spans(tags, offsets):\n",
    "    spans = []\n",
    "    cur_start = None\n",
    "    cur_end = None\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur_start, cur_end\n",
    "        if cur_start is not None and cur_end is not None and cur_end > cur_start:\n",
    "            spans.append((cur_start, cur_end))\n",
    "        cur_start = None\n",
    "        cur_end = None\n",
    "\n",
    "    for (s, e), t in zip(offsets, tags):\n",
    "        if s == 0 and e == 0:   # special tokens\n",
    "            continue\n",
    "        if e <= s:\n",
    "            continue\n",
    "\n",
    "        t0 = t.split(\"-\", 1)[0]  # handles 'B-LINK' too\n",
    "\n",
    "        if t0 == \"U\":\n",
    "            spans.append((s, e))\n",
    "            cur_start = None\n",
    "            cur_end = None\n",
    "        elif t0 == \"B\":\n",
    "            flush()\n",
    "            cur_start, cur_end = s, e\n",
    "        elif t0 == \"I\":\n",
    "            if cur_start is None:\n",
    "                cur_start, cur_end = s, e\n",
    "            else:\n",
    "                cur_end = e\n",
    "        elif t0 == \"L\":\n",
    "            if cur_start is None:\n",
    "                spans.append((s, e))  # broken seq, treat as single\n",
    "            else:\n",
    "                cur_end = e\n",
    "                flush()\n",
    "        else:  # O\n",
    "            flush()\n",
    "\n",
    "    flush()\n",
    "    return spans\n",
    "\n",
    "# build spans\n",
    "pred_spans = bilou_to_spans(tags, offsets)\n",
    "\n",
    "print(\"Pred spans:\", len(pred_spans))\n",
    "print(\"First 10:\")\n",
    "for a, b in pred_spans[:10]:\n",
    "    print(f\"[{a}:{b}] -> {text[a:b]!r}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b26e9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JUNK? (279, 280) '2'\n",
      "JUNK? (283, 284) '3'\n",
      "JUNK? (287, 288) '4'\n",
      "JUNK? (501, 503) 'Ra'\n"
     ]
    }
   ],
   "source": [
    "for a,b in pred_spans:\n",
    "    s = text[a:b]\n",
    "    if len(s) <= 2 or s.isdigit():\n",
    "        print(\"JUNK?\", (a,b), repr(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9987308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible window: 999\n",
      "Gold spans (visible): 15\n",
      "Pred spans (visible): 18\n",
      "TP FP FN: 10 8 5\n",
      "P/R/F1: 0.556 0.667 0.606\n"
     ]
    }
   ],
   "source": [
    "# visible char window from offsets\n",
    "max_visible_char = max(e for (s,e) in offsets if not (s==0 and e==0))\n",
    "\n",
    "gold = set()\n",
    "for s in ex[\"spans\"]:\n",
    "    a=int(s[\"start\"]); b=int(s[\"end\"])\n",
    "    if 0 <= a < b <= max_visible_char:\n",
    "        gold.add((a,b))\n",
    "\n",
    "pred = set((a,b) for (a,b) in pred_spans if b <= max_visible_char)\n",
    "\n",
    "tp = len(gold & pred)\n",
    "fp = len(pred - gold)\n",
    "fn = len(gold - pred)\n",
    "\n",
    "prec = tp/(tp+fp) if tp+fp else 0\n",
    "rec  = tp/(tp+fn) if tp+fn else 0\n",
    "f1   = 2*prec*rec/(prec+rec) if prec+rec else 0\n",
    "\n",
    "print(\"Visible window:\", max_visible_char)\n",
    "print(\"Gold spans (visible):\", len(gold))\n",
    "print(\"Pred spans (visible):\", len(pred))\n",
    "print(\"TP FP FN:\", tp, fp, fn)\n",
    "print(\"P/R/F1:\", round(prec,3), round(rec,3), round(f1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afd87a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-level accuracy: 0.9494\n",
      "Token-level LINK P/R/F1: 0.8824 0.9 0.8911\n",
      "Counts TP/FP/FN: 45 6 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "MAX_LEN = 256\n",
    "\n",
    "# ---- build gold token labels (BILOU per token) ----\n",
    "label_map = {\"O\":0, \"B\":1, \"I\":2, \"L\":3, \"U\":4}   # must match training label ids order\n",
    "id2lab_simple = {v:k for k,v in label_map.items()}\n",
    "\n",
    "def spans_to_token_ids(offsets, gold_spans, ignore_id=-100):\n",
    "    \"\"\"\n",
    "    offsets: list of [start,end] for each token\n",
    "    gold_spans: list of dicts with {\"start\":int,\"end\":int} or tuples\n",
    "    returns: list[int] token label ids aligned to offsets\n",
    "    \"\"\"\n",
    "    # convert gold spans to tuples\n",
    "    gs = []\n",
    "    for s in gold_spans:\n",
    "        if isinstance(s, dict):\n",
    "            gs.append((int(s[\"start\"]), int(s[\"end\"])))\n",
    "        else:\n",
    "            gs.append((int(s[0]), int(s[1])))\n",
    "\n",
    "    labels = []\n",
    "    for (s,e) in offsets:\n",
    "        if s==0 and e==0:\n",
    "            labels.append(ignore_id)\n",
    "            continue\n",
    "\n",
    "        # check overlap with any gold span (token-level)\n",
    "        inside = None\n",
    "        for (a,b) in gs:\n",
    "            if s >= a and e <= b and b > a:\n",
    "                inside = (a,b)\n",
    "                break\n",
    "\n",
    "        if inside is None:\n",
    "            labels.append(label_map[\"O\"])\n",
    "        else:\n",
    "            a,b = inside\n",
    "            # BILOU for token inside this span\n",
    "            if s == a and e == b:\n",
    "                labels.append(label_map[\"U\"])\n",
    "            elif s == a:\n",
    "                labels.append(label_map[\"B\"])\n",
    "            elif e == b:\n",
    "                labels.append(label_map[\"L\"])\n",
    "            else:\n",
    "                labels.append(label_map[\"I\"])\n",
    "    return labels\n",
    "\n",
    "# ---- encode ----\n",
    "text = ex[\"text\"]\n",
    "enc = tok(text, return_offsets_mapping=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "offsets = enc[\"offset_mapping\"][0].tolist()\n",
    "attn = enc[\"attention_mask\"][0].tolist()\n",
    "\n",
    "# gold token labels\n",
    "gold_ids = spans_to_token_ids(offsets, ex[\"spans\"], ignore_id=-100)\n",
    "\n",
    "# pred token labels\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids=enc[\"input_ids\"], attention_mask=enc[\"attention_mask\"]).logits\n",
    "pred_ids = logits.argmax(-1)[0].tolist()\n",
    "\n",
    "# mask: only real tokens (attn=1) and not specials (offset != (0,0))\n",
    "mask = [(attn[i]==1 and not (offsets[i][0]==0 and offsets[i][1]==0) and gold_ids[i]!=-100) for i in range(len(offsets))]\n",
    "\n",
    "gold = np.array([gold_ids[i] for i,m in enumerate(mask) if m], dtype=int)\n",
    "pred = np.array([pred_ids[i] for i,m in enumerate(mask) if m], dtype=int)\n",
    "\n",
    "# token accuracy\n",
    "acc = float((gold==pred).mean()) if len(gold) else 0.0\n",
    "\n",
    "# token F1 for \"LINK\" tokens vs O (binary)\n",
    "gold_pos = (gold != label_map[\"O\"])\n",
    "pred_pos = (pred != label_map[\"O\"])\n",
    "\n",
    "tp = int((gold_pos & pred_pos).sum())\n",
    "fp = int((~gold_pos & pred_pos).sum())\n",
    "fn = int((gold_pos & ~pred_pos).sum())\n",
    "\n",
    "prec = tp/(tp+fp) if tp+fp else 0.0\n",
    "rec  = tp/(tp+fn) if tp+fn else 0.0\n",
    "f1   = 2*prec*rec/(prec+rec) if prec+rec else 0.0\n",
    "\n",
    "print(\"Token-level accuracy:\", round(acc,4))\n",
    "print(\"Token-level LINK P/R/F1:\", round(prec,4), round(rec,4), round(f1,4))\n",
    "print(\"Counts TP/FP/FN:\", tp, fp, fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fandom-span",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
